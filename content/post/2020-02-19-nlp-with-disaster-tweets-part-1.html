---
title: 'NLP with Disaster Tweets: Part 1 Data Preparation'
author: Aditya Mangal
date: '2020-02-19'
categories:
  - Projects
tags:
  - DataAnalysis
  - DataScience
  - NLP
  - TextMining
slug: nlp-with-disaster-tweets-part-1
keywords:
  - tech
summary: NLP and TidyModelling in R
readingtime: '10'
draft: no
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: top
comments: no
coverCaption: Photo by Marcus Kauffman on Unsplash
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#load-libraries">Load Libraries</a></li>
<li><a href="#read-data">Read Data</a></li>
<li><a href="#getting-glove-embedding-for-tweet-text-and-adding-as-features">Getting glove embedding for tweet text and adding as features</a><ul>
<li><a href="#tokenize-incoming-tweet-texts-in-the-training-data">Tokenize incoming tweet texts in the training data</a></li>
<li><a href="#download-and-parse-glove-embeddings-into-an-embedding-matrix-for-the-tokenized-words">Download and parse glove embeddings into an embedding matrix for the tokenized words</a></li>
<li><a href="#generate-embeddings-vector-for-tweets-text-in-training-data">Generate embeddings vector for tweets text in training data</a></li>
<li><a href="#generate-embeddings-vector-for-tweets-text-in-test-data">Generate embeddings vector for tweets text in test data</a></li>
<li><a href="#append-to-given-tweets-features-and-export">Append to given tweets features and export</a></li>
</ul></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this <a href="https://www.kaggle.com/c/nlp-getting-started">NLP getting started challenge</a> on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.<br />
In this part 1 for data preparation, I will do some basic exploration and vectorize the given tweet text into <a href="https://nlp.stanford.edu/projects/glove/">glove</a> embedding vectors.</p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<div id="load-libraries" class="section level2">
<h2>Load Libraries</h2>
<pre class="r"><code>rm(list = ls())
library(tidyverse)
library(ggplot2)
library(GGally)
library(skimr)
library(tidymodels)
library(keras)
library(janitor)

theme_set(theme_light())</code></pre>
</div>
<div id="read-data" class="section level2">
<h2>Read Data</h2>
<pre class="r"><code>tweets &lt;- read_csv(&quot;../data/nlp_with_disaster_tweets/train.csv&quot;)
tweets_test &lt;- read_csv(&quot;../data/nlp_with_disaster_tweets/test.csv&quot;)
skim(tweets)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">tweets</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">7613</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">keyword</td>
<td align="right">61</td>
<td align="right">0.99</td>
<td align="right">4</td>
<td align="right">21</td>
<td align="right">0</td>
<td align="right">221</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">location</td>
<td align="right">2534</td>
<td align="right">0.67</td>
<td align="right">1</td>
<td align="right">49</td>
<td align="right">0</td>
<td align="right">3279</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">text</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">7</td>
<td align="right">157</td>
<td align="right">0</td>
<td align="right">7503</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">id</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5441.93</td>
<td align="right">3137.12</td>
<td align="right">1</td>
<td align="right">2734</td>
<td align="right">5408</td>
<td align="right">8146</td>
<td align="right">10873</td>
<td align="left">▇▇▇▇▇</td>
</tr>
<tr class="even">
<td align="left">target</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.43</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">▇▁▁▁▆</td>
</tr>
</tbody>
</table>
</div>
<div id="getting-glove-embedding-for-tweet-text-and-adding-as-features" class="section level2">
<h2>Getting glove embedding for tweet text and adding as features</h2>
<p>The simple workflow for vectorizing tweet text into glove embeddings is as follows -</p>
<ol style="list-style-type: decimal">
<li>Tokenize incoming tweet texts in the training data.<br />
</li>
<li>Download and parse glove embeddings into an embedding matrix for the tokenized words.<br />
</li>
<li>Generate embeddings vector for tweets text in training data.<br />
</li>
<li>Generate embeddings vector for tweets text in test data.<br />
</li>
<li>Append to given tweets features and export.</li>
</ol>
<div id="tokenize-incoming-tweet-texts-in-the-training-data" class="section level3">
<h3>Tokenize incoming tweet texts in the training data</h3>
<p>Using keras’ text_tokenizer to tokenize the text in tweets dataset.</p>
<pre class="r"><code>num_words &lt;- 10000

text_tokenizer(num_words = num_words) %&gt;% 
  fit_text_tokenizer(tweets$text) -&gt; tokenizer

length(tokenizer$word_index)</code></pre>
<pre><code>## [1] 22700</code></pre>
<p>A total of 22,700 unique words were assigned an index in the tokenization.<br />
Using the above fit tokenizer, converting the text to actual sequences of indices.</p>
<pre class="r"><code>sequences &lt;- texts_to_sequences(tokenizer, tweets$text)

summary(map_int(sequences, length))</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00   11.00   15.00   15.17   19.00   32.00</code></pre>
<pre class="r"><code>maxlen &lt;- 20</code></pre>
<p>Capping the maximum length of a tweets sequence to 20. This will translate all the tweets text sequences into a sequence of length 20. If the original sequence was longer, it will truncate from the beginning and if the original sequence is smaller, it will pad the sequence in the beginning to bring the final length to 20.</p>
<pre class="r"><code>pad_sequences(sequences, maxlen = maxlen) -&gt; padded_sequences

str(padded_sequences)</code></pre>
<pre><code>##  int [1:7613, 1:20] 0 0 1620 0 0 0 0 0 0 0 ...</code></pre>
<p>Like we see above, for all the 7,613 tweets in the training data we have created a tokenized sequence of 20 elements each.</p>
</div>
<div id="download-and-parse-glove-embeddings-into-an-embedding-matrix-for-the-tokenized-words" class="section level3">
<h3>Download and parse glove embeddings into an embedding matrix for the tokenized words</h3>
<p>Downloaded the pre-trained glove embeddings trained on 2 billion tweets from Stanford’s NLP projects page on <a href="https://nlp.stanford.edu/projects/glove/">Glove</a> and borrowing the code for parsing and generating glove embedding matrix from my <a href="https://adityamangal410.github.io/deepSentimentR/reference/index.html">deepSentimentR</a> package.</p>
<pre class="r"><code>parse_glove_embeddings &lt;- function(file_path) {
  lines &lt;- readLines(file_path)
  embeddings_index &lt;- new.env(hash = TRUE, parent = emptyenv())
  for (i in 1:length(lines)) {
    line &lt;- lines[[i]]
    values &lt;- strsplit(line, &quot; &quot;)[[1]]
    word &lt;- values[[1]]
    embeddings_index[[word]] &lt;- as.double(values[-1])
  }
  cat(&quot;Found&quot;, length(embeddings_index), &quot;word vectors.\n&quot;)
  return(embeddings_index)
}

generate_embedding_matrix &lt;- function(word_index, embedding_dim, max_words, glove_file_path) {
  embeddings_index &lt;- parse_glove_embeddings(glove_file_path)

  embedding_matrix &lt;- array(0, c(max_words, embedding_dim))
  for (word in names(word_index)) {
    index &lt;- word_index[[word]]
    if (index &lt; max_words) {
      embedding_vector &lt;- embeddings_index[[word]]
      if (!is.null(embedding_vector)) {
        embedding_matrix[index+1,] &lt;- embedding_vector
      }
    }
  }

  return(embedding_matrix)
}</code></pre>
<pre class="r"><code>embedding_dim &lt;- 25</code></pre>
<pre class="r"><code>embedding_matrix &lt;- generate_embedding_matrix(tokenizer$word_index, 
                                              embedding_dim = embedding_dim, 
                                              max_words = num_words,
&quot;../../../nlp_with_disaster_tweets/data/glove.twitter.27B/glove.twitter.27B.25d.txt&quot;)

saveRDS(embedding_matrix, &quot;../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds&quot;)</code></pre>
<pre class="r"><code>embedding_matrix &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds&quot;)
str(embedding_matrix)</code></pre>
<pre><code>##  num [1:10000, 1:25] 0 0.7864 0.4186 0.7086 -0.0102 ...</code></pre>
<p>Using only 25 dimensional embedding in order to keep the computations fast, we have created an embedding matrix which holds the 25 dimension values for the most popular 10,000 words in our tweets text data.</p>
</div>
<div id="generate-embeddings-vector-for-tweets-text-in-training-data" class="section level3">
<h3>Generate embeddings vector for tweets text in training data</h3>
<p>Using the keras modelling framework to generate embeddings for the given training data. We basically create a simple sequential model with one embedding layer whose weights we will freeze based on our embedding matrix created above, and a flattening layer that will flatten the output into a 2D matrix of dimensions (7613, 20x25).</p>
<pre class="r"><code>keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = num_words, output_dim = embedding_dim, 
                  input_length = maxlen, name = &quot;embedding&quot;) %&gt;% 
  layer_flatten(name = &quot;flatten&quot;) -&gt; model_embedding

model_embedding %&gt;% 
  get_layer(name = &quot;embedding&quot;) %&gt;% 
  set_weights(list(embedding_matrix)) %&gt;% 
  freeze_weights()

model_embedding %&gt;% 
  predict(padded_sequences) -&gt; tweets_embeddings

str(tweets_embeddings)</code></pre>
<pre><code>##  num [1:7613, 1:500] 0 0 -0.524 0 0 ...</code></pre>
<p>For each of the 7,613 padded tweet sequences of upto max length 20, we use the keras model to “predict” and populate the embedding for each of those 20 words in the sequence and susequently flatten those to create a single feature vector of 20x25=500 dimensions.</p>
</div>
<div id="generate-embeddings-vector-for-tweets-text-in-test-data" class="section level3">
<h3>Generate embeddings vector for tweets text in test data</h3>
<p>Using the similar approach as above (i.e. tokenize, pad and vectorize using glove embeddings) on the test data, to generate similar embedding vector for text in the tweets test data. Note that, we use the previously fit text tokenizer on the train data to tokenize the test data.</p>
<pre class="r"><code>sequences_test &lt;- texts_to_sequences(tokenizer, tweets_test$text)</code></pre>
<pre class="r"><code>pad_sequences(sequences_test, maxlen = maxlen) -&gt; padded_sequences_test

model_embedding %&gt;% 
  predict(padded_sequences_test) -&gt; tweets_embeddings_test

str(tweets_embeddings_test)</code></pre>
<pre><code>##  num [1:3263, 1:500] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<p>We similarly get 500 embedding dimensions for 3,263 tweets in the test data.</p>
</div>
<div id="append-to-given-tweets-features-and-export" class="section level3">
<h3>Append to given tweets features and export</h3>
<pre class="r"><code>tweets %&gt;% 
  bind_cols(as_tibble(tweets_embeddings)) %&gt;% 
  clean_names() -&gt; tweets_proc

tweets_test %&gt;% 
  bind_cols(as_tibble(tweets_embeddings_test)) %&gt;% 
  clean_names() -&gt; tweets_test_proc</code></pre>
<pre class="r"><code>saveRDS(tweets_proc, &quot;../data/nlp_with_disaster_tweets/tweets_proc.rds&quot;)
saveRDS(tweets_test_proc, &quot;../data/nlp_with_disaster_tweets/tweets_test_proc.rds&quot;)</code></pre>
<p>Exporting the appended feature set will help us work on this dataset for modelling. I will cover the modelling using the tidymodels framework in my upcoming posts.</p>
</div>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li>Project Summary Page - <a href="/2020/02/nlp-with-disaster-tweets/">NLP with disaster tweets: Summary</a></li>
<li>GloVe: Global Vectors for Word Representation - <a href="https://nlp.stanford.edu/projects/glove/">Stanford NLP Glove Project</a></li>
<li>DeepSentimentR package - <a href="https://adityamangal410.github.io/deepSentimentR/index.html">deepSentimentR</a></li>
</ul>
</div>
