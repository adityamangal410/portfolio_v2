---
title: 'NLP with Disaster Tweets: Part 1 Data Preparation'
author: Aditya Mangal
date: '2020-02-19'
categories:
  - Projects
tags:
  - DataAnalysis
  - DataScience
  - NLP
  - TextMining
slug: nlp-with-disaster-tweets-part-1
keywords:
  - tech
summary: NLP and TidyModelling in R
readingtime: '10'
draft: no
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: left
comments: yes
coverCaption: Photo by Marcus Kauffman on Unsplash
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,error = FALSE)
```

# Introduction

In this [NLP getting started challenge](https://www.kaggle.com/c/nlp-getting-started){target="_blank"} on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.   
In this part 1 for data preparation, I will do some basic exploration and vectorize the given tweet text into [glove](https://nlp.stanford.edu/projects/glove/){target="_blank"} embedding vectors. 

# Analysis

## Load Libraries

```{r}
rm(list = ls())
library(tidyverse)
library(ggplot2)
library(GGally)
library(skimr)
library(tidymodels)
library(keras)
library(janitor)

theme_set(theme_light())
```

## Read Data

```{r}
tweets <- read_csv("../data/nlp_with_disaster_tweets/train.csv")
tweets_test <- read_csv("../data/nlp_with_disaster_tweets/test.csv")
skim(tweets)
```


## Getting glove embedding for tweet text and adding as features

The simple workflow for vectorizing tweet text into glove embeddings is as follows -     
 
 1. Tokenize incoming tweet texts in the training data.  
 2. Download and parse glove embeddings into an embedding matrix for the tokenized words.  
 3. Generate embeddings vector for tweets text in training data.  
 4. Generate embeddings vector for tweets text in test data.   
 5. Append to given tweets features and export.  

### Tokenize incoming tweet texts in the training data

Using keras' text_tokenizer to tokenize the text in tweets dataset. 

```{r}
text_tokenizer() %>% 
  fit_text_tokenizer(tweets$text) -> tokenizer

num_words <- length(tokenizer$word_index) + 1
print(length(tokenizer$word_index))
```

A total of `r length(tokenizer$word_index)` unique words were assigned an index in the tokenization.   
Using the above fit tokenizer, converting the text to actual sequences of indices.

```{r}
sequences <- texts_to_sequences(tokenizer, tweets$text)

summary(map_int(sequences, length))

maxlen <- max(map_int(sequences, length))
print(maxlen)
```

Capping the maximum length of a tweets sequence to `r maxlen`. This will translate all the tweets text sequences into a sequence of length `r maxlen`. If the original sequence was longer, it will truncate from the beginning and if the original sequence is smaller, it will pad the sequence in the beginning to bring the final length to `r maxlen`. 

```{r}
pad_sequences(sequences, maxlen = maxlen) -> padded_sequences

str(padded_sequences)
```

Like we see above, for all the 7,613 tweets in the training data we have created a tokenized sequence of `r maxlen` elements each. 

### Download and parse glove embeddings into an embedding matrix for the tokenized words

Downloaded the pre-trained glove embeddings trained on 2 billion tweets from Stanford's NLP projects page on [Glove](https://nlp.stanford.edu/projects/glove/) and borrowing the code for parsing and generating glove embedding matrix from my [deepSentimentR](https://adityamangal410.github.io/deepSentimentR/reference/index.html) package.

```{r eval=FALSE}
parse_glove_embeddings <- function(file_path) {
  lines <- readLines(file_path)
  embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
  for (i in 1:length(lines)) {
    line <- lines[[i]]
    values <- strsplit(line, " ")[[1]]
    word <- values[[1]]
    embeddings_index[[word]] <- as.double(values[-1])
  }
  cat("Found", length(embeddings_index), "word vectors.\n")
  return(embeddings_index)
}

generate_embedding_matrix <- function(word_index, embedding_dim, max_words, glove_file_path) {
  embeddings_index <- parse_glove_embeddings(glove_file_path)

  embedding_matrix <- array(0, c(max_words, embedding_dim))
  for (word in names(word_index)) {
    index <- word_index[[word]]
    if (index < max_words) {
      embedding_vector <- embeddings_index[[word]]
      if (!is.null(embedding_vector)) {
        embedding_matrix[index+1,] <- embedding_vector
      }
    }
  }

  return(embedding_matrix)
}
```

```{r}
embedding_dim <- 25
```

```{r eval=FALSE}
embedding_matrix <- generate_embedding_matrix(tokenizer$word_index, 
                                              embedding_dim = embedding_dim, 
                                              max_words = num_words,
"../../../nlp_with_disaster_tweets/data/glove.twitter.27B/glove.twitter.27B.25d.txt")

saveRDS(embedding_matrix, "../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds")
```

```{r}
embedding_matrix <- readRDS("../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds")
str(embedding_matrix)
```

Using only 25 dimensional embedding in order to keep the computations fast, we have created an embedding matrix which holds the 25 dimension values for the most popular `r length(tokenizer$word_index)` words in our tweets text data. 

### Generate embeddings vector for tweets text in training data

Using the keras modelling framework to generate embeddings for the given training data. We basically create a simple sequential model with one embedding layer whose weights we will freeze based on our embedding matrix created above, and a flattening layer that will flatten the output into a 2D matrix of dimensions (7613, `r maxlen`x25).

```{r}
keras_model_sequential() %>% 
  layer_embedding(input_dim = num_words, output_dim = embedding_dim, 
                  input_length = maxlen, name = "embedding") %>% 
  layer_flatten(name = "flatten") -> model_embedding

model_embedding %>% 
  get_layer(name = "embedding") %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model_embedding %>% 
  predict(padded_sequences) -> tweets_embeddings

str(tweets_embeddings)
```

For each of the 7,613 padded tweet sequences of upto max length `r maxlen`, we use the keras model to "predict" and populate the embedding for each of those `r maxlen` words in the sequence and susequently flatten those to create a single feature vector of `r maxlen`x25=825 dimensions.

### Generate embeddings vector for tweets text in test data

Using the similar approach as above (i.e. tokenize, pad and vectorize using glove embeddings) on the test data, to generate similar embedding vector for text in the tweets test data. Note that, we use the previously fit text tokenizer on the train data to tokenize the test data.

```{r}
sequences_test <- texts_to_sequences(tokenizer, tweets_test$text)
```


```{r}
pad_sequences(sequences_test, maxlen = maxlen) -> padded_sequences_test

model_embedding %>% 
  predict(padded_sequences_test) -> tweets_embeddings_test

str(tweets_embeddings_test)
```

We similarly get 825 embedding dimensions for 3,263 tweets in the test data. 

### Append to given tweets features and export

```{r}
tweets %>% 
  bind_cols(as_tibble(tweets_embeddings)) %>% 
  clean_names() -> tweets_proc

tweets_test %>% 
  bind_cols(as_tibble(tweets_embeddings_test)) %>% 
  clean_names() -> tweets_test_proc
```

```{r eval=FALSE}
saveRDS(tweets_proc, "../data/nlp_with_disaster_tweets/tweets_proc.rds")
saveRDS(tweets_test_proc, "../data/nlp_with_disaster_tweets/tweets_test_proc.rds")
```

Exporting the appended feature set will help us work on this dataset for modelling. I will cover the modelling using the tidymodels framework in my upcoming posts. 

# References

 - Project Summary Page - [NLP with disaster tweets: Summary](/2020/02/nlp-with-disaster-tweets/){target="_blank"}
 - GloVe: Global Vectors for Word Representation - [Stanford NLP Glove Project](https://nlp.stanford.edu/projects/glove/){target="_blank"}
 - DeepSentimentR package - [deepSentimentR](https://adityamangal410.github.io/deepSentimentR/index.html){target="_blank"}
 
 