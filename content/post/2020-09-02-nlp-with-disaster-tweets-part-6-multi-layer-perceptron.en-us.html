---
title: 'NLP with Disaster Tweets: Part 6 Multi Layer Perceptron'
author: Aditya Mangal
date: '2020-09-02'
slug: nlp-with-disaster-tweets-part-6-multi-layer-perceptron.en-us
categories:
  - Projects
tags:
  - DataScience
  - DeepLearning
  - NLP
  - Python
  - TextMining
keywords:
  - tech
summary: Multi Layer Perceptron using PyTorch
readingtime: '8'
draft: no
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: top
comments: yes
coverCaption: Photo by Marcus Kauffman on Unsplash
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#classifier-class">Classifier Class</a><ul>
<li><a href="#network-architecture-and-loss-function">Network Architecture and Loss Function</a></li>
<li><a href="#optimizer">Optimizer</a></li>
<li><a href="#forward-method">Forward Method</a></li>
<li><a href="#training-step-val-step-test-step">Training Step / Val Step / Test Step</a></li>
</ul></li>
<li><a href="#the-training-routine">The Training Routine</a></li>
<li><a href="#inference-and-results">Inference and Results</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this <a href="https://www.kaggle.com/c/nlp-getting-started" target="_blank">NLP getting started challenge</a> on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.<br />
In this part 6 for building Multi Layer Perceptron, I will use the data module generated in <a href="/2020/08/nlp-with-disaster-tweets-part-5-deep-learning-data-preparation.en-us/" target="_blank">Part 5</a> to create a Multi Layer Perceptron model to predict if the tweet is about a real disaster. Following up from the previous <a href="/2020/04/nlp-with-disaster-tweets-part-4-tree-based-models.en-us/" target="_blank">Part 4</a> about tree-based models, I will general the prediction output of this model on the validation set and compare results.</p>
</div>
<div id="classifier-class" class="section level1">
<h1>Classifier Class</h1>
<p>I will be using the <a href="https://github.com/PyTorchLightning/pytorch-lightning" target="_blank">pytorch-lightning</a> framework to build a classifier class.<br />
In a deep learning training pipeline we need to configure the following 5 essential components -</p>
<ul>
<li>Network architecture - Exactly how is the network structured? Is it a perceptron? a convolutional network? a recurrent network? How many layers does it have? How exactly each layer is laid out? Number of neurons in case of perceptron, number of channels in case of convolutional network etc.</li>
<li>Loss function - How do we calculate the loss of the prediction from the truth and backpropogate it’s gradient through the network so as to update the weights?</li>
<li>Optimizer - Optimizers update the weight parameters to minimize the loss function. In its simplest form, we multiply our preconfigured constant learning rate to the gradient of each parameter in the network to update it. Fancier optimizers update the learning rate, bring in momentum etc. but essentially accomplish the same goal of updating the weights to minimize loss.</li>
<li>Forward Pass - This method defines how the training batch will forward pass through the Network by using parameter weights and activation functions (and dropout/batch norms) and finally generate the prediction output.</li>
<li>Training Step / Val Step / Test Step - We configure specific to each phase (train/val/test) how exactly we will compute and report the loss.</li>
</ul>
<p>That’s it! Rest everything related to epoch loop, checkpointing, logging, book-keeping etc. are handled by PyTorch Lightning so we do not need to implement that specifically (as we would have if we were to use vanilla PyTorch. Example <a href="/2020/05/from-tensorflow-to-pytorch.en-us" target="_blank">here</a>). In the rest of the post I will mainly discuss the above 5 components specific to building a Multi Layer Perceptron for our Disaster Tweets usecase.</p>
<div id="network-architecture-and-loss-function" class="section level2">
<h2>Network Architecture and Loss Function</h2>
<p>A multi layer perceptron is a simple neural network where neurons in each layer are fully connected to all the neurons in the next layer. Weights are applied to each input feature by a linear transform along with a bias and non-linearity is introduced via the activation function. Let’s see how our network architecture and loss function look.</p>
<pre class="python"><code>import pytorch_lightning as pl

class DisasterTweetsClassifierMLP(pl.LightningModule):

    def __init__(self, num_classes, dropout_p, pretrained_embeddings,
                 max_seq_length, learning_rate, weight_decay):
        super().__init__()

        self.save_hyperparameters(&#39;num_classes&#39;, &#39;dropout_p&#39;, &#39;learning_rate&#39;, &#39;weight_decay&#39;)

        embedding_dim = pretrained_embeddings.size(1)
        num_embeddings = pretrained_embeddings.size(0)

        self.emb = torch.nn.Embedding(embedding_dim=embedding_dim,
                                      num_embeddings=num_embeddings,
                                      padding_idx=0,
                                      _weight=pretrained_embeddings)
        self.fc = torch.nn.Sequential(
            torch.nn.Linear(in_features=embedding_dim * max_seq_length, out_features=128),
            torch.nn.BatchNorm1d(num_features=128),
            torch.nn.Dropout(p=dropout_p),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=128, out_features=64),
            torch.nn.BatchNorm1d(num_features=64),
            torch.nn.Dropout(p=dropout_p),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=64, out_features=32),
            torch.nn.BatchNorm1d(num_features=32),
            torch.nn.Dropout(p=dropout_p),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=32, out_features=16),
            torch.nn.BatchNorm1d(num_features=16),
            torch.nn.Dropout(p=dropout_p),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=16, out_features=num_classes)
        )
        self.loss = torch.nn.CrossEntropyLoss(reduction=&#39;none&#39;)</code></pre>
<ul>
<li>We are using the LightningModule abstraction provided by PyTorch Lightning to create our training module.</li>
<li>First we do some housekeeping by saving all the hyperparameters that are passed to our module.</li>
<li>As we discussed in my previous post about <a href="/2020/08/nlp-with-disaster-tweets-part-5-deep-learning-data-preparation.en-us/" target="_blank">data preparation</a>, we will use pretrained glove embeddings to embed our vectorized tweets into lower dimensional vectors. For this we use the <code>torch.nn.Embedding</code> layer with pretrained embeddings supplied as initial weights.</li>
<li>Next we define our <code>fc</code> (fully connected) network, using the <code>torch.nn.Sequential</code> syntactic sugar provided by PyTorch. In here we have 5 fully connected layers of 128, 64, 32, 16 and 2 neurons respectively. Each of these layers use linear transform (therefore <code>torch.nn.Linear</code>) to transform its input from one dimension to another.</li>
<li>In order to regularize the network so it doesn’t overfit our training data we are using Dropout, this will randomly drop output of certain nodes/neurons based on given dropout probability. <a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" target="_blank">Here</a> is an excellent post on Dropout if you are curious.</li>
<li>In order to stabilize the learning process and reduce the number of training epochs required for the network to converge, we are also using batch normalization via <code>torch.nn.BatchNorm1d</code>. This will standardize (0 mean and 1 standard deviation) the inputs to a layer for each batch. <a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/" target="_blank">Here</a> is an excellent post on Batch Normalization if you are curious.</li>
<li>We apply ReLU activation to the output of each layer which provides us with our non-linearity. It will keep the output as is if it is positive or else make it zero.</li>
<li>Finally, the last layer will provide 2 outputs, one for each of our classes (disaster or not). We can transform it to probability by using softmax to see what’s the predicted probability of each class.<br />
</li>
<li>Loss Function - We are using the Cross Entropy Loss function here. My favorite explanation for this loss is <a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" target="_blank">this</a> post that does a beautiful job of explaining how exactly it is computed and how its gradient is calculated. The reason for passing ‘reduction=none’ in the loss function will become clear in the section below about training/val step.</li>
</ul>
</div>
<div id="optimizer" class="section level2">
<h2>Optimizer</h2>
<pre class="python"><code>class DisasterTweetsClassifierMLP(pl.LightningModule):
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(),
                                lr=self.hparams.learning_rate,
                                weight_decay=self.hparams.weight_decay)</code></pre>
<p>We are using the adam optimizer for optimizer our network parameters which is a good default optimizer as it combines advantages of 2 other well known optimizers AdaGrad and RMSProp. <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" target="_blank">Here</a> is a gentle introduction on Adam optimizer. We pass in all our network parameters, the provided learning rate and weight decay hyper parameters to <code>torch.optim.Adam</code>.</p>
</div>
<div id="forward-method" class="section level2">
<h2>Forward Method</h2>
<pre class="python"><code>class DisasterTweetsClassifierMLP(pl.LightningModule):
  def forward(self, batch):
        x_embedded = self.emb(batch).view(batch.size(0), -1)
        output = self.fc(x_embedded)

        return output</code></pre>
<p>In the forward pass through the network, we first generate the embeddings for our batch and also flatten it. For example, if we had batch size as 8 and each of the 8 samples were of normalized length 70, then the <code>self.emb</code> embedding layer will transform it to a tensor 8x70x25 (if we were to use 25 dimensional glove embeddings). i.e. it’ll extract the 25-dimensional embedding for each of 70 words in our 8 samples. Then the <code>view(batch.size(0), -1)</code> will flatten it such that the tensor will transform from 8x70x25 to 8x1750 (i.e. it’ll append the embedding vectors of all 70 words in the sample for all 8 samples).<br />
This flattened vector then gets passed through our <code>fc</code> fully connected layer and generates the output which gets returned.</p>
</div>
<div id="training-step-val-step-test-step" class="section level2">
<h2>Training Step / Val Step / Test Step</h2>
<pre class="python"><code>class DisasterTweetsClassifierMLP(pl.LightningModule):
  def training_step(self, batch, batch_idx):
        y_pred = self(batch[&#39;x_data&#39;]) #internally calls the forward method
        loss = self.loss(y_pred, batch[&#39;y_target&#39;]).mean()
        return {&#39;loss&#39;: loss, &#39;log&#39;: {&#39;train_loss&#39;: loss}}</code></pre>
<p>In our training step, one batch of training dataset is passed to the class (i.e. the forward method). The output is then passed, along with the true target values, to our loss function. We return the mean cross entropy loss across all samples in the batch. Note that, we also pass a <code>log</code> field in our response which is used by our logger utility for training instrumentation. In our case, we use the Tensorboard Logger (more about that ahead).</p>
<pre class="python"><code>class DisasterTweetsClassifierMLP(pl.LightningModule):
  def validation_step(self, batch, batch_idx):
        y_pred = self(batch[&#39;x_data&#39;])
        loss = self.loss(y_pred, batch[&#39;y_target&#39;])
        acc = (y_pred.argmax(-1) == batch[&#39;y_target&#39;]).float()
        return {&#39;loss&#39;: loss, &#39;acc&#39;: acc}
        
  def validation_epoch_end(self, outputs):
        loss = torch.cat([o[&#39;loss&#39;] for o in outputs], 0).mean()
        acc = torch.cat([o[&#39;acc&#39;] for o in outputs], 0).mean()
        out = {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc}
        return {**out, &#39;log&#39;: out}</code></pre>
<p>We have split the validation step in 2 methods. The <code>validation_step</code> and the <code>validation_epoch_end</code>. Since we want to summarize the loss for the entire validation dataset and not just for each individual validation batch, we only return the individual losses of each sample in the batch from our validation_step method (similar to how we computed in training step) and do the aggregation in the <code>validation_epoch_end</code> method, which as the name suggests runs after the validation epoch is over. Here, we see the reason for passing <code>reduction=none</code> in our loss function instantiation. Since, we don’t allow the loss function to do reduction, and handle it manually ourselves, we can use the same function for both train and validation steps. As we noted earlier, training loss gets computed over each batch and gets backpropogated, however validation loss is computed over the entire validation dataset and reported. We also compute accuracy on our validation set for reporting purposes.</p>
<p>The test step is exactly identical to the validation step (Mutatis mutandis). i.e. Test dataset used instead of validation dataset.</p>
</div>
</div>
<div id="the-training-routine" class="section level1">
<h1>The Training Routine</h1>
<p>With all the above work, our network is all set and ready to train. Let’s discuss the training routine which remains mostly same across different networks. It majorly has class object creation, logger configuration and running the <code>fit</code> method on our trainer.</p>
<pre class="python"><code>if __name__ == &#39;__main__&#39;:
  pl.seed_everything(42)

  dm = DisasterTweetsDataModule(tweets_data_path=args.tweets_data_path,
                                embeddings_path=args.embeddings_path,
                                batch_size=args.batch_size,
                                num_workers=args.num_workers)

  dm.setup(&#39;fit&#39;)

  model = DisasterTweetsClassifierMLP(num_classes=2,
                                      dropout_p=args.dropout_p,
                                      learning_rate=args.learning_rate,
                                      weight_decay=args.weight_decay,
                                      pretrained_embeddings=dm.pretrained_embeddings,
                                      max_seq_length=dm.train_ds.get_max_seq_length())

  trainer = pl.Trainer.from_argparse_args(args,
                                          deterministic=True,
                                          weights_summary=&#39;full&#39;,
                                          early_stop_callback=True)

  trainer.configure_logger(pl.loggers.TensorBoardLogger(&#39;lightning_logs/&#39;,
                                                        name=&#39;disaster_tweets_mlp_glove&#39;))
  trainer.fit(model, dm)</code></pre>
<ul>
<li>We first seed everything so we can have reproducible results.</li>
<li>Next, we create an object of our DisasterTweetsDataModule passing all the required params and also call it’s setup method. (For more context checkout my previous post <a href="/2020/08/nlp-with-disaster-tweets-part-5-deep-learning-data-preparation.en-us/" target="_blank">Part 5</a>).</li>
<li>We create our DisasterTweetsClassifierMLP model object which holds all the above code related to the network, loss, optimizer etc.</li>
<li>After that we instantiate the <code>Trainer</code> object provided by PyTorch Lightning, which runs our entire training pipeline and provides a lot of other functionality like logging and early stopping. For now, we have enabled the default early stopping callback, which will stop the training if we do not see <code>val_loss</code> decreasing for 3 consecutive epochs.</li>
<li>We also configure the TensorBoard Logger that will help visualize and monitor the training routine.</li>
<li>Finally, we call the <code>fit</code> method of the trainer with the Model object and the Lightning Data Module object.</li>
</ul>
</div>
<div id="inference-and-results" class="section level1">
<h1>Inference and Results</h1>
<div class="figure">
<img src="https://lh3.googleusercontent.com/pw/ACtC-3cxsz2O5JYhPo9QiqUx6DfcWzA6MuP6mOSS9eVw-lSuK0NhaagIN1NxnZQwEy-GDwldRy-UqDBsZV1nDKtbzbHRwvaP1gXhuFomirKsCaED29flV6r7tP8tH5dAmrpa5oF0EuIQZJXCUvMQzG5adFtGKQ=w452-h1044-no?authuser=0" alt="" />
<p class="caption">Training Routine Plots in Tensorboard</p>
</div>
<p>The above plots show the training routine visualizations in Tensorboard. We see the validation loss continuously decreases until around epoch=13 where it starts saturating and our early stopping callback gets triggered and stops the training.</p>
<pre class="python"><code>def predict_target(text, classifier, vectorizer, max_seq_length):
    text_vector, _ = vectorizer.vectorize(text, max_seq_length)
    text_vector = torch.tensor(text_vector)
    pred = torch.nn.functional.softmax(classifier(text_vector.unsqueeze(dim=0)), dim=1)
    probability, target = pred.max(dim=1)

    return {&#39;pred&#39;: target.item(), &#39;probability&#39;: probability.item()}</code></pre>
<p>We can use the above method to run the trained model on new incoming text and generate predictions. When we run prediction on our validation dataset we get an accuracy score of <code>0.7835232252410167</code> and F1-score of <code>0.7507568113017156</code>. F1-score for the best tuned Gradient Boosted Tree model that we got in <a href="/2020/04/nlp-with-disaster-tweets-part-4-tree-based-models.en-us/#gradient-boosted-trees-using-xgboost" target="_blank">Part 4</a> was <code>0.8107538</code>, which is much higher than our multi-layer perceptron. However, note that this is just the default performance of this Network Architecture on our validation set with single constant values for our different hyperparameters like batch size, learning rate, hidden layers etc. We can do a full hyperparameter sweep and tune this network to get an optimal performance number (coming up in a future post).</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>In this part of the series, we built a multi-layer perceptron neural network architecture to predict if a tweet is about a real disaster using the PyTorch Lightning framework. We built this on top of the Lightning Data Module that we discussed in the previous post of this series. We can see that we get a good solid baseline performance from this network and can hopefully with further hyperparameter tuning improve it. The full code for this post can be viewed on my github <a href="https://github.com/adityamangal410/nlp_with_pytorch/blob/master/scripts/3.2-am-pl-mlp-glove-disaster-tweets.py" target="_blank">here</a>. In the next part, I will build a Convolutional Neural Network for this task using our same underlying data module. Hope to see you there.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li>Project Summary Page - <a href="/2020/02/nlp-with-disaster-tweets/" target="_blank">NLP with disaster tweets: Summary</a></li>
<li>Project Part 1 - <a href="/2020/02/nlp-with-disaster-tweets-part-1/" target="_blank">NLP with Disaster Tweets: Part 1 Data Preparation</a></li>
<li>Project Part 2 - <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">NLP with Disaster Tweets: Part 2 Nearest Neighbor Models</a></li>
<li>Project Part 3 - <a href="/2020/04/nlp-with-disaster-tweets-part-3-linear-models/" target="_blank">NLP with Disaster Tweets: Part 3 Linear Models</a></li>
<li>Project Part 4 - <a href="/2020/04/nlp-with-disaster-tweets-part-4-tree-based-models.en-us/" target="_blank">NLP with Disaster Tweets: Part 4 Tree-based Models</a></li>
<li>Project Part 5 - <a href="/2020/08/nlp-with-disaster-tweets-part-5-deep-learning-data-preparation.en-us/" target="_blank">NLP with Disaster Tweets: Part 5 Deep Learning Data Preparation</a></li>
<li>PyTorch - <a href="https://pytorch.org/docs/stable/index.html" target="_blank">Docs</a></li>
<li>PyTorch Lightning - <a href="https://pytorch-lightning.readthedocs.io/en/latest/" target="_blank">Docs</a></li>
<li>Natural Language Processing with PyTorch - <a href="https://learning.oreilly.com/library/view/natural-language-processing/9781491978221/" target="_blank">ebook</a></li>
<li>Full DisasterTweetsClassifierMLP implementation - <a href="https://github.com/adityamangal410/nlp_with_pytorch/blob/master/scripts/3.2-am-pl-mlp-glove-disaster-tweets.py" target="_blank">github</a></li>
</ul>
</div>
