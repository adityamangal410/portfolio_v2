---
title: 'NLP with Disaster Tweets: Part 3 Linear Models'
author: Aditya Mangal
date: '2020-04-06'
categories:
  - Projects
tags:
  - DataAnalysis
  - DataScience
  - NLP
  - TextMining
slug: nlp-with-disaster-tweets-part-3-linear-models
keywords:
  - tech
summary: NLP and Tidymodelling in R
readingtime: '15'
draft: no
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: left
comments: yes
coverCaption: Photo by Marcus Kauffman on Unsplash
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#load-libraries">Load Libraries</a></li>
<li><a href="#loading-processed-data-from-previous-part">Loading processed data from previous part</a></li>
<li><a href="#feature-preprocessing-and-engineering">Feature preprocessing and engineering</a></li>
<li><a href="#modelling">Modelling</a><ul>
<li><a href="#lasso-regression-using-glmnet">Lasso Regression using glmnet</a></li>
</ul></li>
<li><a href="#model-calibration">Model Calibration</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this <a href="https://www.kaggle.com/c/nlp-getting-started" target="_blank">NLP getting started challenge</a> on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.<br />
In this part 3 for Linear Modelling, I will use the processed data generated in <a href="/2020/02/nlp-with-disaster-tweets-part-1/" target="_blank">Part 1</a> to train logistic regression based model with regularization in order to predict if a tweet is about a real disaster or not using the tidymodels framework. Following up from the previous <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">Part 2</a> about Nearest Neighbors model, I will do a comparative study among these 2 modelling algorithms.</p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<div id="load-libraries" class="section level2">
<h2>Load Libraries</h2>
<pre class="r"><code>rm(list = ls())
library(tidyverse)
library(ggplot2)
library(tidymodels)
library(glmnet)
library(vip)
library(silgelib)

theme_set(theme_plex())</code></pre>
</div>
<div id="loading-processed-data-from-previous-part" class="section level2">
<h2>Loading processed data from previous part</h2>
<pre class="r"><code>tweets &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/tweets_proc.rds&quot;)
tweets_final &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/tweets_test_proc.rds&quot;)</code></pre>
<pre class="r"><code>tweets %&gt;% 
  dim</code></pre>
<pre><code>## [1] 7613  830</code></pre>
<pre class="r"><code>tweets_final %&gt;% 
  dim</code></pre>
<pre><code>## [1] 3263  829</code></pre>
</div>
<div id="feature-preprocessing-and-engineering" class="section level2">
<h2>Feature preprocessing and engineering</h2>
<p>I will use the same steps of feature preprocessing and engineering from <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">Part 2</a> here, in order to have an apples to apples comparison of the 2 models.</p>
<pre class="r"><code>tweets %&gt;% 
  mutate(target = as.factor(target),
         id = as.character(id)) -&gt; tweets

set.seed(42)
tweets_split &lt;- initial_split(tweets, prop = 0.1, strata = target)

tweets_test &lt;- training(tweets_split)
tweets_train_cv &lt;- testing(tweets_split)

set.seed(42)
tweets_split &lt;- initial_split(tweets_train_cv, prop = 7/9, strata = target)
tweets_train &lt;- training(tweets_split)
tweets_cv &lt;- testing(tweets_split)

recipe(target ~ ., data = tweets_train) %&gt;% 
  update_role(id, new_role = &quot;ID&quot;) %&gt;% 
  step_rm(location, keyword) %&gt;% 
  step_mutate(len = str_length(text),
              num_hashtags = str_count(text, &quot;#&quot;)) %&gt;% 
  step_rm(text) %&gt;% 
  step_zv(all_numeric(), -all_outcomes()) %&gt;% 
  step_normalize(all_numeric(), -all_outcomes())  %&gt;% 
  step_pca(all_predictors(), -len, -num_hashtags, threshold = 0.80)-&gt; tweets_recipe

tweets_prep &lt;- tweets_recipe %&gt;% 
  prep(training = tweets_train, 
       strings_as_factors = FALSE)</code></pre>
</div>
<div id="modelling" class="section level2">
<h2>Modelling</h2>
<div id="lasso-regression-using-glmnet" class="section level3">
<h3>Lasso Regression using glmnet</h3>
<p>Let’s build a basic logistic regression model with some default value of regularization penalty.</p>
<div id="basic" class="section level4">
<h4>Basic</h4>
<pre class="r"><code>lasso_spec &lt;- logistic_reg(penalty = 0.1, mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;) %&gt;% 
  set_mode(&quot;classification&quot;) 

wf &lt;- workflow() %&gt;% 
  add_recipe(tweets_recipe)</code></pre>
<pre class="r"><code>lasso_fit &lt;- wf %&gt;% 
  add_model(lasso_spec) %&gt;% 
  fit(data = tweets_train)

saveRDS(lasso_fit, &quot;../data/nlp_with_disaster_tweets/glmnet/lasso_basic_fit.rds&quot;)</code></pre>
<pre class="r"><code>lasso_fit &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/glmnet/lasso_basic_fit.rds&quot;)

lasso_fit %&gt;% 
  pull_workflow_fit() %&gt;% 
  tidy() </code></pre>
<pre><code>## # A tibble: 11,684 x 5
##    term         step estimate lambda dev.ratio
##    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)     1  -0.283  0.151   1.80e-14
##  2 (Intercept)     2  -0.284  0.138   1.17e- 2
##  3 PC001           2  -0.0101 0.138   1.17e- 2
##  4 (Intercept)     3  -0.284  0.126   2.14e- 2
##  5 PC001           3  -0.0194 0.126   2.14e- 2
##  6 (Intercept)     4  -0.285  0.114   2.97e- 2
##  7 PC001           4  -0.0280 0.114   2.97e- 2
##  8 (Intercept)     5  -0.286  0.104   3.66e- 2
##  9 PC001           5  -0.0360 0.104   3.66e- 2
## 10 (Intercept)     6  -0.288  0.0951  5.21e- 2
## # … with 11,674 more rows</code></pre>
<p>Above we see the familiar output of glmnet. Now, Let’s try and tune the regularization penalty and visualize the results.</p>
</div>
<div id="tuning-lasso-penalty" class="section level4">
<h4>Tuning lasso penalty</h4>
<p>Using 10-fold cross validation and a few different values of regularization penalty.</p>
<pre class="r"><code>set.seed(1234)
folds &lt;- vfold_cv(tweets_train, strata = target, v = 10, repeats = 3)

tune_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;glmnet&quot;)

lambda_grid &lt;- grid_regular(penalty(), levels = 75)</code></pre>
<pre class="r"><code>doParallel::registerDoParallel(cores = parallel::detectCores(logical = FALSE))

set.seed(1234)
lasso_grid &lt;- tune_grid(
  wf %&gt;% add_model(tune_spec),
  resamples = folds,
  grid = lambda_grid,
  metrics = metric_set(accuracy, roc_auc, f_meas),
  control = control_grid(save_pred = TRUE,
                           verbose = TRUE)
)

saveRDS(lasso_grid, &quot;../data/nlp_with_disaster_tweets/glmnet/lasso_grid.rds&quot;)</code></pre>
<pre class="r"><code>lasso_grid &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/glmnet/lasso_grid.rds&quot;)

lasso_grid %&gt;% 
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 225 x 6
##     penalty .metric  .estimator  mean     n std_err
##       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1 1.00e-10 accuracy binary     0.772    30 0.00343
##  2 1.00e-10 f_meas   binary     0.807    30 0.00277
##  3 1.00e-10 roc_auc  binary     0.836    30 0.00370
##  4 1.37e-10 accuracy binary     0.772    30 0.00343
##  5 1.37e-10 f_meas   binary     0.807    30 0.00277
##  6 1.37e-10 roc_auc  binary     0.836    30 0.00370
##  7 1.86e-10 accuracy binary     0.772    30 0.00343
##  8 1.86e-10 f_meas   binary     0.807    30 0.00277
##  9 1.86e-10 roc_auc  binary     0.836    30 0.00370
## 10 2.54e-10 accuracy binary     0.772    30 0.00343
## # … with 215 more rows</code></pre>
<pre class="r"><code>lasso_grid %&gt;% 
  collect_metrics() %&gt;% 
  mutate(flexibility = 1/penalty,
         .metric = str_to_title(str_replace_all(.metric, &quot;_&quot;, &quot; &quot;))) %&gt;% 
  ggplot(aes(flexibility, mean, color = .metric)) + 
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err), alpha = 0.5) + 
  geom_line(size = 1.5) + 
  facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 3) + 
  scale_x_log10() + 
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Model performance against model flexibility&quot;,
       subtitle = &quot;F1-score peaks around lower flexibility values&quot;,
       x = &quot;Model flexibility i.e. Log(1/penalty)&quot;,
       y = &quot;Mean metric value&quot;)</code></pre>
<p><img src="/post/2020-04-06-nlp-with-disaster-tweets-part-3-linear-models_files/figure-html/unnamed-chunk-11-1.png" width="672" />
As we see in the plot above, the f1-score increases on the evaluation set until around penalty=0.005 and then starts falling down. We plot the flexibility (i.e. 1/penalty) to visualize how the model performance varies as the model flexibility increases. The Lasso Regression model with a lower penalty value will be highly flexible since it will not penalize the features weights by a lot and thus have high variance, whereas a higher penalty would lead to a much stricter model which is less flexible since a lot of feature weights will be moved to zero due to high penalty and therefore the model might suffer from bias.</p>
<p>Let’s pickout the best parameter penalty based on the highest f1-score and train our final model on the full training dataset and evaluate against cross validation dataset.</p>
<pre class="r"><code>lasso_grid %&gt;% 
  select_best(&quot;f_meas&quot;) -&gt; highest_f_meas

final_lasso &lt;- finalize_workflow(
  wf %&gt;% add_model(tune_spec),
  highest_f_meas
)</code></pre>
<pre class="r"><code>last_fit(final_lasso, 
         tweets_split,
         metrics = metric_set(accuracy, roc_auc, f_meas)) -&gt; lasso_last_fit

saveRDS(lasso_last_fit, &quot;../data/nlp_with_disaster_tweets/glmnet/lasso_last_fit.rds&quot;)</code></pre>
<pre class="r"><code>lasso_last_fit &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/glmnet/lasso_last_fit.rds&quot;)
lasso_last_fit %&gt;% 
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.763
## 2 f_meas   binary         0.806
## 3 roc_auc  binary         0.826</code></pre>
<p>The f1-score metric on the validation set comes out to be 0.8058252 which is much higher compared to the baseline and also the K-Nearest Neighbor model that we built in <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">Part 2</a>. Lets try and visualize the variable importance for different features that we used to train our model.</p>
<pre class="r"><code>lasso_last_fit$.workflow[[1]] %&gt;% 
  pull_workflow_fit() %&gt;% 
  vi(lambda = highest_f_meas$penalty) %&gt;% 
  mutate(Importance = abs(Importance),
         Variable = fct_reorder(Variable, Importance)) %&gt;% 
  top_n(10, Importance) %&gt;% 
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) + 
  geom_col() + 
  scale_x_continuous(expand = c(0,0)) + 
  labs(y = NULL,
       x = &quot;Importance&quot;,
       title = &quot;Variable Importance plot&quot;,
       subtitle = &quot;&#39;len&#39; feature has high positive impact on disaster probability&quot;)</code></pre>
<p><img src="/post/2020-04-06-nlp-with-disaster-tweets-part-3-linear-models_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Since most of our features are the dimensionally reduced different dimensions coming out of the glove embeddings, we can not interpret them. However, as in the plot above, our custom feature “len” i.e. length of the tweets comes out as one of the most important features. The model gives a higher probability of a tweet being about a disaster if its longer in length, signifying that people tweeting about real disasters usually use a lot more words as compared to others.</p>
<p>Let’s visualize how the coefficient estimates vary based on lambda for our 2 custom features, length of tweets and number of hashtags, and also for 1 other high coefficient feature in both positive and negative directions each.</p>
<pre class="r"><code>lasso_last_fit$.workflow[[1]] %&gt;% 
  pull_workflow_fit() %&gt;% 
  tidy() %&gt;% 
  filter(term %in% c(&quot;len&quot;, &quot;num_hashtags&quot;, &quot;PC009&quot;, &quot;PC026&quot;)) %&gt;% 
  select(term, estimate, lambda) %&gt;% 
  pivot_wider(names_from = term, values_from = estimate) %&gt;% 
  replace_na(list(num_hashtags = 0, PC009 = 0, len = 0, PC026 = 0)) %&gt;% 
  pivot_longer(names_to = &quot;term&quot;, values_to = &quot;estimate&quot;, cols = -lambda) %&gt;% 
  ggplot(aes(x = lambda, y = estimate, color = term)) + 
  geom_point() + 
  geom_line() +
  geom_abline(slope = 0, intercept = 0, linetype = 2, 
            color = &#39;grey50&#39;) +
  scale_x_log10() + 
  labs(x = &quot;Log (Penalty Parameter) &quot;,
       y = &quot;Coefficient Estimate&quot;,
       color = &quot;Feature&quot;,
       title = &quot;Feature coefficients against penalty parameter&quot;,
       subtitle = &quot;&#39;num_hashtags&#39; feature gets to zero coefficient very soon in comparison&quot;)</code></pre>
<p><img src="/post/2020-04-06-nlp-with-disaster-tweets-part-3-linear-models_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>All feature coefficients tend to move towards zero as we increase the lambda penalty parameter, some more quickly than others.</p>
</div>
</div>
</div>
<div id="model-calibration" class="section level2">
<h2>Model Calibration</h2>
<p>Another method to measure model performance is to see how well calibrated a model is on the validation dataset. Let’s compare our 2 models, lasso regression here and the best k-nearest neighbor model from <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">Part 2</a>, by plotting calibration plots for each of them.</p>
<pre class="r"><code>readRDS(&quot;../data/nlp_with_disaster_tweets/knn/knn_last_fit.rds&quot;) %&gt;% 
  collect_predictions() %&gt;% 
  mutate(pred_bucket = as.integer(`.pred_1`/(1/11))) %&gt;% 
  group_by(pred_bucket) %&gt;% 
  summarise(mean_truth = mean(as.numeric(as.character(target)))*100,
            mean_pred = mean(`.pred_1`)*100) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(x = mean_pred, y = mean_truth)) +
  geom_point(color = &quot;#F8766D&quot;, size = 3) + 
  geom_line(color = &quot;#F8766D&quot;) +
  scale_x_continuous(limits = c(0, 100)) +
  scale_y_continuous(limits = c(0, 100)) +
  geom_abline(slope = 1, intercept = 0, linetype = 2, 
            color = &#39;grey50&#39;) +
  labs(x = &quot;Mean True Prediction&quot;,
       y = &quot;Mean Truth&quot;,
       title = &quot;Calibration Plot for K-nearest neighbors model&quot;,
       subtitle = &quot;Above the diagonal signifies under-forecast&quot;)</code></pre>
<p><img src="/post/2020-04-06-nlp-with-disaster-tweets-part-3-linear-models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>lasso_last_fit %&gt;% 
  collect_predictions() %&gt;% 
  mutate(pred_bucket = as.integer(`.pred_1`/(1/11))) %&gt;% 
  group_by(pred_bucket) %&gt;% 
  summarise(mean_truth = mean(as.numeric(as.character(target)))*100,
            mean_pred = mean(`.pred_1`)*100) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(x = mean_pred, y = mean_truth)) +
  geom_point(color = &quot;#F8766D&quot;, size = 3) + 
  geom_line(color = &quot;#F8766D&quot;) +
  scale_x_continuous(limits = c(0, 100)) +
  scale_y_continuous(limits = c(0, 100)) +
  geom_abline(slope = 1, intercept = 0, linetype = 2, 
            color = &#39;grey50&#39;) +
  labs(x = &quot;Mean True Prediction&quot;,
       y = &quot;Mean Truth&quot;,
       title = &quot;Calibration Plot for Lasso Regression model&quot;,
       subtitle = &quot;Close to the diagonal signifies well calibrated model&quot;)</code></pre>
<p><img src="/post/2020-04-06-nlp-with-disaster-tweets-part-3-linear-models_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>In general, closer the calibration curve to the diagonal, the better. When a model is well calibrated, the mean prediction probability will be equal to mean truth. Meaning the model is predicting the target in the same ratios as the actual truth values.<br />
The KNN model calibration plot shows that the curve is above the diagonal, i.e. the model is frequently under estimating the predicted probabilities.<br />
However, the lasso regression seems to be well calibrated out of the box. That’s a sign of good stable algorithm and training procedure.<br />
Note that all the predicted probabilities here are calculated on the cross validation set.</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>In the next part of this series, I will start building models with a different class of function space, like piece-wise constant functions and linear combination of piece-wise constant functions classes i.e. simple decision trees and gradient boosted trees respectively. As we see in this blog post that the class of linear functions do give better performing models and we learnt our problem space to be leaning towards flexible models in <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">Part 2</a>, I expect to see better results for different, more flexible modelling approaches.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li>Project Summary Page - <a href="/2020/02/nlp-with-disaster-tweets/" target="_blank">NLP with disaster tweets: Summary</a></li>
<li>Project Part 1 - <a href="/2020/02/nlp-with-disaster-tweets-part-1/" target="_blank">NLP with Disaster Tweets: Part 1 Data Preparation</a></li>
<li>Project Part 2 - <a href="/2020/02/nlp-with-disaster-tweets-part-2-nearest-neighbor-models/" target="_blank">NLP with Disaster Tweets: Part 2 Nearest Neighbor Models</a></li>
<li>Lasso Regression using <a href="https://juliasilge.com/blog/lasso-the-office/" target="_blank">Tidymodels by Julia Silge</a>.</li>
<li>Introduction to Statistical Learning - <a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370" target="_blank">Book</a></li>
</ul>
</div>
