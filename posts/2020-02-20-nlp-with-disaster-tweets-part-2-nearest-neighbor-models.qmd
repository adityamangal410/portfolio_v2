---
title: 'NLP with Disaster Tweets: Part 2 Nearest Neighbor Models'
author: Aditya Mangal
date: '2020-02-20'
slug: nlp-with-disaster-tweets-part-2-nearest-neighbor-models
categories:
  - Projects
tags:
  - DataAnalysis
  - DataScience
  - NLP
  - TextMining
keywords:
  - tech
summary: NLP and Tidymodelling in R
readingtime: '15'
draft: false
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: left
coverCaption: Photo by Marcus Kauffman on Unsplash
format:
  html:
    theme: distill
freeze: true
output-file: nlp-with-disaster-tweets-part-2-nearest-neighbor-models.html
---




# Introduction
<p>In this <a href="https://www.kaggle.com/c/nlp-getting-started" target="_blank">NLP getting started challenge</a> on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.<br />
In this part 2 for Nearest Neighbor Modelling, I will use the processed data generated in <a href="/2020/02/nlp-with-disaster-tweets-part-1/" target="_blank">Part 1</a> to train nearest neighbor models in order to predict if a tweet is about a real disaster or not using the tidymodels framework.</p>
# Analysis
## Load Libraries
<pre class="r"><code>rm(list = ls())
library(tidyverse)
library(ggplot2)
library(tidymodels)
library(silgelib)

theme_set(theme_plex())</code></pre>
## Loading processed data from previous part
<pre class="r"><code>tweets &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/tweets_proc.rds&quot;)
tweets_final &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/tweets_test_proc.rds&quot;)</code></pre>
<pre class="r"><code>tweets %&gt;% 
  dim</code></pre>
<pre><code>## [1] 7613  830</code></pre>
<pre class="r"><code>tweets_final %&gt;% 
  dim</code></pre>
<pre><code>## [1] 3263  829</code></pre>
## Feature preprocessing and engineering
<pre class="r"><code>tweets %&gt;% 
  mutate(target = as.factor(target),
         id = as.character(id)) -&gt; tweets</code></pre>
<pre class="r"><code>tweets %&gt;% 
  count(target, sort = T)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   target     n
##   &lt;fct&gt;  &lt;int&gt;
## 1 0       4342
## 2 1       3271</code></pre>
### Split data
<p>Splitting the data into 3 sets. A test set of 10% data, a cross validation set of 20% data and a training set of 70% data. Training and validation sets will be used for training, tuning and validating performance of models and comparing among them. Test set will only be used for final estimation of the model performance on unknown data.</p>
<pre class="r"><code>set.seed(42)
tweets_split &lt;- initial_split(tweets, prop = 0.1, strata = target)

tweets_test &lt;- training(tweets_split)
tweets_train_cv &lt;- testing(tweets_split)

set.seed(42)
tweets_split &lt;- initial_split(tweets_train_cv, prop = 7/9, strata = target)
tweets_train &lt;- training(tweets_split)
tweets_cv &lt;- testing(tweets_split)</code></pre>
<pre class="r"><code>dim(tweets_train)</code></pre>
<pre><code>## [1] 5328  830</code></pre>
<pre class="r"><code>dim(tweets_cv)</code></pre>
<pre><code>## [1] 1522  830</code></pre>
<pre class="r"><code>dim(tweets_test)</code></pre>
<pre><code>## [1] 763 830</code></pre>
### Preparation Recipe
<p>I will use the recipe package from tidymodels to generate a recipe for data preprocessing and feature engineering.</p>
<pre class="r"><code>recipe(target ~ ., data = tweets_train) %&gt;% 
  update_role(id, new_role = &quot;ID&quot;) %&gt;% 
  step_rm(location, keyword) %&gt;% 
  step_mutate(len = str_length(text),
              num_hashtags = str_count(text, &quot;#&quot;)) %&gt;% 
  step_rm(text) %&gt;% 
  step_zv(all_numeric(), -all_outcomes()) %&gt;% 
  step_normalize(all_numeric(), -all_outcomes())  %&gt;% 
  step_pca(all_predictors(), -len, -num_hashtags, threshold = 0.80)-&gt; tweets_recipe</code></pre>
<p>Note above</p>
<ul>
<li>We use the training dataset to create the recipe<br />
</li>
<li>We won’t use ‘id’ field as a predictor, only as an identifier.<br />
</li>
<li>For current analysis, we will drop the location and keyword features.<br />
</li>
<li>Creating a length feature to model the tweet length and another feature to store the number of hashtags in the tweet.<br />
</li>
<li>Getting rid of the text field since we have generated all the features from it that we wanted for now.<br />
</li>
<li>Removing all predictors with zero variance.<br />
</li>
<li>Normalizing all features i.e. centering and scaling.<br />
</li>
<li>Adding dimensionality reduction using PCA to keep 80% variance and reduce the number of features while still keeping our custom features.</li>
</ul>
<pre class="r"><code>tweets_prep &lt;- tweets_recipe %&gt;% 
  prep(training = tweets_train, 
       strings_as_factors = FALSE)</code></pre>
## Modelling
### Baseline model
<p>I will first create a baseline model to beat. In this case, we can predict randomly in the ratio of target counts and evaluate the model performance accordingly.</p>
<pre class="r"><code>tweets_prep %&gt;% 
  juice() %&gt;% 
  count(target) %&gt;% 
  mutate(prob = n/sum(n)) %&gt;% 
  pull(prob) -&gt; probs</code></pre>
<pre class="r"><code>set.seed(42)
tweets_prep %&gt;% 
  bake(new_data = tweets_cv) %&gt;% 
  mutate(predicted_target = as.factor(sample(0:1, 
                                             size = nrow(tweets_cv), 
                                             prob = probs, replace = T))) %&gt;% 
  accuracy(target, predicted_target)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.512</code></pre>
<pre class="r"><code>set.seed(42)
tweets_prep %&gt;% 
  bake(new_data = tweets_cv) %&gt;% 
  mutate(predicted_target = as.factor(sample(0:1, 
                                             size = nrow(tweets_cv), 
                                             prob = probs, replace = T))) %&gt;% 
  f_meas(target, predicted_target)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.581</code></pre>
<p>Like, we see above, we have a baseline f1 score of 0.5812. We need to build and train a model that beats this baseline.</p>
<pre class="r"><code>set.seed(42)
tweets_prep %&gt;% 
  bake(new_data = tweets_test) %&gt;% 
  mutate(predicted_target = as.factor(sample(0:1, 
                                             size = nrow(tweets_test), 
                                             prob = probs, replace = T))) %&gt;% 
  accuracy(target, predicted_target)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.503</code></pre>
<pre class="r"><code>set.seed(42)
tweets_prep %&gt;% 
  bake(new_data = tweets_test) %&gt;% 
  mutate(predicted_target = as.factor(sample(0:1, 
                                             size = nrow(tweets_test), 
                                             prob = probs, replace = T))) %&gt;% 
  f_meas(target, predicted_target)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.574</code></pre>
<p><em>Generating submission file</em></p>
<pre class="r"><code>set.seed(42)
tweets_prep %&gt;% 
  bake(new_data = tweets_final) %&gt;% 
  mutate(target = as.factor(sample(0:1, 
                                   size = nrow(tweets_final), 
                                   prob = probs, replace = T))) %&gt;% 
  select(id, target) %&gt;% 
  write_csv(&quot;../data/nlp_with_disaster_tweets/submissions/baseline_cvf_57_testf_57.csv&quot;)</code></pre>
### K-Nearest Neighbor model
<p>Let’s build a basic KNN model with some default number of neighbors to see how the modelling is done in this framework and checkout how the modelling output looks like.</p>
#### Basic
<pre class="r"><code>knn_spec &lt;- nearest_neighbor(neighbors = 3) %&gt;% 
  set_engine(&quot;kknn&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)

wf &lt;- workflow() %&gt;% 
  add_recipe(tweets_recipe)</code></pre>
<pre class="r"><code>knn_fit &lt;- wf %&gt;% 
  add_model(knn_spec) %&gt;% 
  fit(data = tweets_train)

saveRDS(knn_fit, &quot;../data/nlp_with_disaster_tweets/knn/knn_basic_fit.rds&quot;)</code></pre>
<pre class="r"><code>knn_fit &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/knn/knn_basic_fit.rds&quot;)
knn_fit %&gt;% 
  pull_workflow_fit() -&gt; wf_fit

wf_fit$fit$MISCLASS</code></pre>
<pre><code>##     optimal
## 3 0.3521021</code></pre>
<p>The above shows a simple K-nearest neighbors model using the “kknn” engine. Gives about 0.3521021 of minimal misclassification.
Let’s try and tune the number of neighbors (k) and see if we can interpret the underlying problem space.</p>
#### Tuning number of neighbors
<p>Using 5-fold cross validation and values of K going from 1 to 100.</p>
<pre class="r"><code>set.seed(1234)
folds &lt;- vfold_cv(tweets_train, strata = target, v = 5, repeats = 1)

tune_spec &lt;- nearest_neighbor(neighbors = tune()) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;kknn&quot;)

neighbor_grid &lt;- expand.grid(neighbors = seq(1,100, by = 1))</code></pre>
<pre class="r"><code>set.seed(1234)
doParallel::registerDoParallel(cores = parallel::detectCores(logical = FALSE))


knn_grid &lt;- tune_grid(
  wf %&gt;% add_model(tune_spec),
  resamples = folds,
  grid = neighbor_grid,
  metrics = metric_set(accuracy, roc_auc, f_meas),
  control = control_grid(save_pred = TRUE,
                           verbose = TRUE)
)

saveRDS(knn_grid, &quot;../data/nlp_with_disaster_tweets/knn/knn_grid.rds&quot;)</code></pre>
<pre class="r"><code>knn_grid &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/knn/knn_grid.rds&quot;)

knn_grid %&gt;% 
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 300 x 6
##    neighbors .metric  .estimator  mean     n std_err
##        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1         1 accuracy binary     0.628     5 0.00597
##  2         1 f_meas   binary     0.705     5 0.00426
##  3         1 roc_auc  binary     0.604     5 0.00653
##  4         2 accuracy binary     0.628     5 0.00598
##  5         2 f_meas   binary     0.705     5 0.00426
##  6         2 roc_auc  binary     0.630     5 0.00928
##  7         3 accuracy binary     0.628     5 0.00544
##  8         3 f_meas   binary     0.706     5 0.00349
##  9         3 roc_auc  binary     0.639     5 0.00873
## 10         4 accuracy binary     0.628     5 0.00537
## # … with 290 more rows</code></pre>
<pre class="r"><code>knn_grid %&gt;% 
  collect_metrics() %&gt;% 
  mutate(flexibility = 1/neighbors,
         .metric = str_to_title(str_replace_all(.metric, &quot;_&quot;, &quot; &quot;))) %&gt;% 
  ggplot(aes(flexibility, mean, color = .metric)) + 
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err), alpha = 0.5) + 
  geom_line(size = 1.5) + 
  facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 3) + 
  scale_x_log10() + 
  theme(legend.position = &quot;none&quot;) + 
  labs(title = &quot;Model performance against model flexibility&quot;,
       subtitle = &quot;F1-score peaks around lower flexibility values&quot;,
       x = &quot;Model flexibility i.e. Log(1/NumberOfNeighbors)&quot;,
       y = &quot;Mean metric value&quot;)</code></pre>
<p><img src="/post/2020-02-20-nlp-with-disaster-tweets-part-2-nearest-neighbor-models_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>As we see in the plot above, the f1-score increases on the evaluation set until around K=20 and then starts falling down. We plot the flexibility (i.e. 1/NumberOfNeighbors) to visualize how the model performance varies as the model flexibility increases. The KNN model with K=1 will be highly flexible and thus have high variance, whereas K=100 would lead to a much stricter model which is less flexible and might suffer from bias.<br />
Looks like our underlying problem stays much closer to being flexible than strict (since optimal K looks to be around 20). We should remember this fact for picking further models for experimentation.</p>
<p>Let’s pickout the best parameter K based on the highest f1-score and train our final model on the full training dataset and evaluate against cross validation dataset.</p>
<pre class="r"><code>knn_grid %&gt;% 
  select_best(&quot;f_meas&quot;) -&gt; highest_f_meas

final_knn &lt;- finalize_workflow(
  wf %&gt;% add_model(tune_spec),
  highest_f_meas
)</code></pre>
<pre class="r"><code>last_fit(final_knn, 
         tweets_split,
         metrics = metric_set(accuracy, roc_auc, f_meas)) -&gt; knn_last_fit

saveRDS(knn_last_fit, &quot;../data/nlp_with_disaster_tweets/knn/knn_last_fit.rds&quot;)</code></pre>
<pre class="r"><code>knn_last_fit &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/knn/knn_last_fit.rds&quot;)
knn_last_fit %&gt;% 
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.643
## 2 f_meas   binary         0.756
## 3 roc_auc  binary         0.687</code></pre>
<p>Our final fit knn model with K=25 gives an f1-score of 0.7560538, which is much higher than our baseline model on the same CV dataset.</p>
# Summary
<p>We can hence learn quite a few things about our underlying problem space by using this basic modelling algorithm K-nearest neighbors and use our learning in further model selection and tuning and also generate a fairly robust model that predicts quite effectively as compared to the baseline model.<br />
Also, this tidymodels framework provides a good modelling structure which can be easily reproduced and used to train a variety of models. In the next part of this series, I will work on another classic modelling algorithm, Lasso Regression, where we will also see if we can identify if there any of these features are much more important than the others and if our 2 custom features are useful.</p>
# References
<ul>
<li>Project Summary Page - <a href="/2020/02/nlp-with-disaster-tweets/" target="_blank">NLP with disaster tweets: Summary</a></li>
<li>Project Part 1 - <a href="/2020/02/nlp-with-disaster-tweets-part-1/" target="_blank">NLP with Disaster Tweets: Part 1 Data Preparation</a></li>
<li>Lasso Regression using <a href="https://juliasilge.com/blog/lasso-the-office/" target="_blank">Tidymodels by Julia Silge</a>.</li>
<li>Introduction to Statistical Learning - <a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370" target="_blank">Book</a></li>
</ul>
