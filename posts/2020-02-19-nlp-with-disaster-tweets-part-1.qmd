---
title: 'NLP with Disaster Tweets: Part 1 Data Preparation'
author: Aditya Mangal
date: '2020-02-19'
categories:
- Projects
keywords:
- tech
- DataAnalysis
- DataScience
- NLP
- TextMining
summary: NLP and TidyModelling in R
readingtime: '10'
draft: false
coverImage: https://lh3.googleusercontent.com/NhQQ6_tTtlSd0SOqdTjXXaaFxEC186pYF68dyx8BJcdeqWUns0kas6nKS7jNUjTorlWCOvYDSXvgTbJSMgHLvc-ViHQkvKe_cGiU4gMP92vb1QSO98BlROygzqVBRO_2pnbPPS3coQVCdbAGJP3PQi0J4hExUkwsxCyPZ7RSOu9kOsIFJ7i7Pi68Ewg8yzEUw7zSb5JyYoDRkunuYedzzrPvS1Z2LWx-T-bVnkedbv72e3fjIfHHpwDf2LM4Tz_orR0lSjyrLt29k8YP2vbGoqRJhOurgj8UxWRC8J1BmDZxaG4fMzF5VfY2QToUTEFUryIzihBzb82z9IBQB7KiBuoKZBvrM03uHBwi_ePO7lt138P5DSufFH3xANhUrY8UPbMIcEKOKaxNXGUhdZG1EDykY6KW5ufjuAgEFoOKUSVp_kHyHmk_qJonukh5zRL07tUhRyXwYNXoP1ixHX-bN8K4yLLobU-Q2kAlrXD89OzMySyD6sm8gu6LcrzwthgEWLwTgRgcpvx0hPncV477w6mZnu1yd8xi8EAE3IVVhWAlQF2oUphPtcWjOBfK-aIocc62MSEFfCPb6DLSBpCWCkFFxydl0KSrbvboXBVX-AM7eVyAnFV3SfSfdQMxtKuLtWbAINTcPhoN05LhQxiOOJtc4KCJFbUOWO29U-rjrunXEbRXPhki1DUCy_KX5phRnwO3hniDAwqubCEZkKU3s6T0yCVT4WARB7E5RcmoVPdLWSRK=w2372-h1832-no
thumbnailImage: https://lh3.googleusercontent.com/-z0Ja7Q6Sb7BXIno6Ycxun1BRzcJ_en9A9EulOKiXX8QAAx_UWYtH8y04Vqu9-YZF9G1zROwyLXxH77yvOKGqlfcCJPpC5S6_0wAEVc7mNLOCq0bmj2r0iE20NP0P_2jvxfZ3TtHAQHvagPsTbe7lbVxhy3-6yEdsAMn4idYDkIbxtgMj5Elo9SeO991NyEDNyj6a31SVlt1U22HnrFhmJMtYvilT30ZCXq3zaC5y7PKMjb1bA605bTRhc0AiDPCUtD8u2AjyrJ0CwaDQ7dzvW-UpeGx-BoJZvveJIPITw7HiofhcCcYoM69W0CDCwDf106GgH8KDRvlnOdlc9AHgqKdwJ38E1Pbxu9enMofV6WsIdx9-a-P0V8UVltiEKu-Iqq0pfnvEVznP3sQlQw6WV1jG2tWrs78xXM4U4Bu2SqOAjQc8rVJaTVh5uOJfon8zxSJ1opZSnzif364UXlZJpsXxouufy6TDEDUxe5c-jI8wEWPNLEy2JeFwjP9FXxm3EnSbRvIUnjtALh2T-ma0Xyd8d7EJQrsE2vziIAK-ZVY6A6qnv9bptefReAq9p5jmBRABp7wFm8NSuPIXdyYneDch1M4V1SSjf4oz7oaP_AuHMuaAKViG65uuw6QoLCNoDsR4Ifq8g4ADUZxp0naty2ElqCkS0Lj3vND0ZcuUMaMUiqUygAcmmInaZMaCqQa-KWRDwgvxL-NCgV1UzXtE81F8nxhcFW481Bjt7fSDQWz8LNm=w750-h579-no
thumbnailImagePosition: left
coverCaption: Photo by Marcus Kauffman on Unsplash
format:
  html:
    theme: distill
freeze: true
output-file: nlp-with-disaster-tweets-part-1.html
---

# Introduction
In this NLP getting started challenge on kaggle, we are given tweets which are classified as 1 if they are about real disasters and 0 if not. The goal is to predict given the text of the tweets and some other metadata about the tweet, if its about a real disaster or not.
In this part 1 for data preparation, I will do some basic exploration and vectorize the given tweet text into glove embedding vectors.
# Analysis
## Load Libraries
```r
rm(list = ls())
library(tidyverse)
library(ggplot2)
library(GGally)
library(skimr)
library(tidymodels)
library(keras)
library(janitor)

theme_set(theme_light())
```
## Read Data
```r
tweets &lt;- read_csv(&quot;../data/nlp_with_disaster_tweets/train.csv&quot;)
tweets_test &lt;- read_csv(&quot;../data/nlp_with_disaster_tweets/test.csv&quot;)
skim(tweets)
```

Table 1: Data summary

Name
tweets

Number of rows
7613

Number of columns
5

_______________________

Column type frequency:

character
3

numeric
2

________________________

Group variables
None

Variable type: character

skim_variable
n_missing
complete_rate
min
max
empty
n_unique
whitespace

keyword
61
0.99
4
21
0
221
0

location
2534
0.67
1
49
0
3279
0

text
0
1.00
7
157
0
7503
0

Variable type: numeric

skim_variable
n_missing
complete_rate
mean
sd
p0
p25
p50
p75
p100
hist

id
0
1
5441.93
3137.12
1
2734
5408
8146
10873
▇▇▇▇▇

target
0
1
0.43
0.50
0
0
0
1
1
▇▁▁▁▆

## Getting glove embedding for tweet text and adding as features
The simple workflow for vectorizing tweet text into glove embeddings is as follows -

Tokenize incoming tweet texts in the training data.

Download and parse glove embeddings into an embedding matrix for the tokenized words.

Generate embeddings vector for tweets text in training data.

Generate embeddings vector for tweets text in test data.

- Append to given tweets features and export.

### Tokenize incoming tweet texts in the training data
Using keras’ text_tokenizer to tokenize the text in tweets dataset.
```r
text_tokenizer() %&gt;%
  fit_text_tokenizer(tweets$text) -&gt; tokenizer

num_words &lt;- length(tokenizer$word_index) + 1
print(length(tokenizer$word_index))
```
```
## [1] 22700
```
A total of 22700 unique words were assigned an index in the tokenization.
Using the above fit tokenizer, converting the text to actual sequences of indices.
```r
sequences &lt;- texts_to_sequences(tokenizer, tweets$text)

summary(map_int(sequences, length))
```
```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
##    1.00   13.00   17.00   16.84   21.00   33.00
```
```r
maxlen &lt;- max(map_int(sequences, length))
print(maxlen)
```
```
## [1] 33
```
Capping the maximum length of a tweets sequence to 33. This will translate all the tweets text sequences into a sequence of length 33. If the original sequence was longer, it will truncate from the beginning and if the original sequence is smaller, it will pad the sequence in the beginning to bring the final length to 33.
```r
pad_sequences(sequences, maxlen = maxlen) -&gt; padded_sequences

str(padded_sequences)
```
```
##  int [1:7613, 1:33] 0 0 0 0 0 0 0 0 0 0 ...
```
Like we see above, for all the 7,613 tweets in the training data we have created a tokenized sequence of 33 elements each.
### Download and parse glove embeddings into an embedding matrix for the tokenized words
Downloaded the pre-trained glove embeddings trained on 2 billion tweets from Stanford’s NLP projects page on Glove and borrowing the code for parsing and generating glove embedding matrix from my deepSentimentR package.
```r
parse_glove_embeddings &lt;- function(file_path) {
  lines &lt;- readLines(file_path)
  embeddings_index &lt;- new.env(hash = TRUE, parent = emptyenv())
  for (i in 1:length(lines)) {
    line &lt;- lines[[i]]
    values &lt;- strsplit(line, &quot; &quot;)[[1]]
    word &lt;- values[[1]]
    embeddings_index[[word]] &lt;- as.double(values[-1])
  }
  cat(&quot;Found&quot;, length(embeddings_index), &quot;word vectors.\n&quot;)
  return(embeddings_index)
}

generate_embedding_matrix &lt;- function(word_index, embedding_dim, max_words, glove_file_path) {
  embeddings_index &lt;- parse_glove_embeddings(glove_file_path)

  embedding_matrix &lt;- array(0, c(max_words, embedding_dim))
  for (word in names(word_index)) {
    index &lt;- word_index[[word]]
    if (index &lt; max_words) {
      embedding_vector &lt;- embeddings_index[[word]]
      if (!is.null(embedding_vector)) {
        embedding_matrix[index+1,] &lt;- embedding_vector
      }
    }
  }

  return(embedding_matrix)
}
```
```r
embedding_dim &lt;- 25
```
```r
embedding_matrix &lt;- generate_embedding_matrix(tokenizer$word_index,
                                              embedding_dim = embedding_dim,
                                              max_words = num_words,
&quot;../../../nlp_with_disaster_tweets/data/glove.twitter.27B/glove.twitter.27B.25d.txt&quot;)

saveRDS(embedding_matrix, &quot;../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds&quot;)
```
```r
embedding_matrix &lt;- readRDS(&quot;../data/nlp_with_disaster_tweets/embedding_matrix_25d.rds&quot;)
str(embedding_matrix)
```
```
##  num [1:22701, 1:25] 0 0.7864 0.4186 0.7086 -0.0102 ...
```
Using only 25 dimensional embedding in order to keep the computations fast, we have created an embedding matrix which holds the 25 dimension values for the most popular 22700 words in our tweets text data.
### Generate embeddings vector for tweets text in training data
Using the keras modelling framework to generate embeddings for the given training data. We basically create a simple sequential model with one embedding layer whose weights we will freeze based on our embedding matrix created above, and a flattening layer that will flatten the output into a 2D matrix of dimensions (7613, 33x25).
```r
keras_model_sequential() %&gt;%
  layer_embedding(input_dim = num_words, output_dim = embedding_dim,
                  input_length = maxlen, name = &quot;embedding&quot;) %&gt;%
  layer_flatten(name = &quot;flatten&quot;) -&gt; model_embedding

model_embedding %&gt;%
  get_layer(name = &quot;embedding&quot;) %&gt;%
  set_weights(list(embedding_matrix)) %&gt;%
  freeze_weights()

model_embedding %&gt;%
  predict(padded_sequences) -&gt; tweets_embeddings

str(tweets_embeddings)
```
```
##  num [1:7613, 1:825] 0 0 0 0 0 0 0 0 0 0 ...
```
For each of the 7,613 padded tweet sequences of upto max length 33, we use the keras model to “predict” and populate the embedding for each of those 33 words in the sequence and susequently flatten those to create a single feature vector of 33x25=825 dimensions.
### Generate embeddings vector for tweets text in test data
Using the similar approach as above (i.e. tokenize, pad and vectorize using glove embeddings) on the test data, to generate similar embedding vector for text in the tweets test data. Note that, we use the previously fit text tokenizer on the train data to tokenize the test data.
```r
sequences_test &lt;- texts_to_sequences(tokenizer, tweets_test$text)
```
```r
pad_sequences(sequences_test, maxlen = maxlen) -&gt; padded_sequences_test

model_embedding %&gt;%
  predict(padded_sequences_test) -&gt; tweets_embeddings_test

str(tweets_embeddings_test)
```
```
##  num [1:3263, 1:825] 0 0 0 0 0 0 0 0 0 0 ...
```
We similarly get 825 embedding dimensions for 3,263 tweets in the test data.
### Append to given tweets features and export
```r
tweets %&gt;%
  bind_cols(as_tibble(tweets_embeddings)) %&gt;%
  clean_names() -&gt; tweets_proc

tweets_test %&gt;%
  bind_cols(as_tibble(tweets_embeddings_test)) %&gt;%
  clean_names() -&gt; tweets_test_proc
```
```r
saveRDS(tweets_proc, &quot;../data/nlp_with_disaster_tweets/tweets_proc.rds&quot;)
saveRDS(tweets_test_proc, &quot;../data/nlp_with_disaster_tweets/tweets_test_proc.rds&quot;)
```
Exporting the appended feature set will help us work on this dataset for modelling. I will cover the modelling using the tidymodels framework in my upcoming posts.
# References

- Project Summary Page - NLP with disaster tweets: Summary
- GloVe: Global Vectors for Word Representation - Stanford NLP Glove Project
- DeepSentimentR package - deepSentimentR

